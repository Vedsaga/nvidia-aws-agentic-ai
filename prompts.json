{
  "kriya_extraction_prompt": "You are a semantic role labeling expert. Extract verbs and their semantic roles.\n\nSemantic roles (kārakas):\n- KARTA: The agent/doer (who performs the action)\n- KARMA: The patient/object (what is acted upon)\n- KARANA: The instrument (tool/means used)\n- SAMPRADANA: The recipient (who receives something)\n- APADANA: The source (where something comes from)\n- ADHIKARANA_SPATIAL: Physical location\n- ADHIKARANA_TEMPORAL: Time reference\n\nReturn ONLY valid JSON in this format:\n{\"extractions\":[{\"verb\":\"action\",\"karakas\":{\"KARTA\":\"agent\",\"KARMA\":\"patient\"},\"coreferences\":[]}]}\n\nRules:\n- Include only karakas present in the sentence\n- For compound agents (\"X and Y\"), keep them together as KARTA\n- coreferences: use [] if none\n- NO null values, NO extra braces, NO trailing commas\n\nExamples:\n\nInput: \"John gave Mary a book.\"\nOutput: {\"extractions\":[{\"verb\":\"gave\",\"karakas\":{\"KARTA\":\"John\",\"KARMA\":\"book\",\"SAMPRADANA\":\"Mary\"},\"coreferences\":[]}]}\n\nInput: \"Alice and Bob reviewed the report.\"\nOutput: {\"extractions\":[{\"verb\":\"reviewed\",\"karakas\":{\"KARTA\":\"Alice and Bob\",\"KARMA\":\"report\"},\"coreferences\":[]}]}",
  "kriya_extraction_feedback_prompt": "Previous attempt failed: {feedback}\n\nFix these issues and generate a NEW extraction:\n\n1. Extract ALL verbs (if multiple actions exist)\n2. KARTA = agent/doer (who does it)\n3. KARMA = patient/object (what is acted upon)\n4. Keep compound agents together (\"X and Y\" as one KARTA)\n5. Don't hallucinate - only extract what's in the text\n6. Return valid JSON with extractions array\n\nCommon mistakes to avoid:\n- Swapping agent and patient\n- Splitting compound agents\n- Adding entities not in text\n- Invalid JSON syntax",
  "kriya_scoring_prompt": "Score this semantic role extraction from 1-100.\n\nCheck:\n1. Verb correct? (20 pts)\n2. KARTA (agent) correct? (20 pts)\n3. KARMA (patient) correct? (20 pts)\n4. Other roles correct? (20 pts)\n5. JSON valid? (10 pts)\n6. Nothing hallucinated? (10 pts)\n\nCommon errors:\n- Agent labeled as patient\n- Missing compound agents (\"X and Y\")\n- Wrong verb tense\n- Hallucinated entities\n\nReturn JSON:\n{\"score\":85,\"reasoning\":\"Verb correct, KARTA correct, but KARMA missing\"}\n\nBe strict. Deduct 20 points per major error.",
  "kriya_verification_prompt": "Pick the BEST extraction or return ALL_INVALID.\n\nYou'll receive:\n- original_text: the sentence to extract from\n- candidates: list of extractions with id and extraction data\n\nCheck each candidate:\n1. All entities (KARTA, KARMA, etc.) must exist in original_text - no hallucinations\n2. Verb must match the action in original_text\n3. KARTA = who does the action, KARMA = what is acted upon\n\nPick the candidate with correct entities and roles. If all have errors, return ALL_INVALID.\n\nReturn ONLY JSON:\n{\"choice\":\"Candidate_A\"}\n\nOr if all wrong:\n{\"choice\":\"ALL_INVALID\",\"reasoning\":\"brief reason\"}",
  "query_decomposition_prompt": "You are a query planning expert for knowledge graph traversal. Decompose the user query into a multi-hop graph traversal plan.\n\nThe graph has:\n- Node types: Kriyā (actions), Entity (agents/objects), Document (source text)\n- Edge types: HAS_KARTĀ, HAS_KARMA, USES_KARANA, TARGETS_SAMPRADĀNA, FROM_APĀDĀNA, LOCATED_IN, OCCURS_AT, IS_SAME_AS, CAUSES, CITED_IN\n\nReturn JSON with this structure:\n{\n  \"steps\": [\n    {\n      \"step_number\": 1,\n      \"description\": \"<what to find>\",\n      \"verb\": \"<target verb or null>\",\n      \"karakas\": {\n        \"KARTA\": \"<entity constraint or null>\",\n        \"KARMA\": \"<entity constraint or null>\"\n      },\n      \"follow_causes\": <true|false>,\n      \"follow_same_as\": <true|false>\n    }\n  ],\n  \"expected_hops\": <integer>,\n  \"reasoning\": \"<explanation of plan>\"\n}\n\nEach step should be executable as a graph traversal operation.",
  "query_scoring_prompt": "You are an expert evaluator of query execution plans. Score the following query plan on a scale of 1-100.\n\nEvaluation criteria:\n- Query understanding accuracy (25 points)\n- Graph traversal logic correctness (35 points)\n- Multi-hop reasoning completeness (20 points)\n- Executability (can it actually run on the graph?) (20 points)\n\nReturn JSON with this exact structure:\n{\n  \"reasoning\": \"<detailed explanation of score>\",\n  \"score\": <integer between 1 and 100>\n}\n\nBe strict. Score must be an integer from 1 to 100.",
  "query_verification_prompt": "You are a query plan verifier. Check if the query plan is logically sound and executable on a knowledge graph.\n\nVerify:\n1. Each step has clear graph traversal operations\n2. Edge types used are valid (HAS_KARTĀ, HAS_KARMA, CAUSES, etc.)\n3. Multi-hop logic makes sense for the question\n4. Plan doesn't have circular dependencies\n5. Expected result type matches question (yes/no, entity, narrative, etc.)\n\nGiven multiple candidate plans, select the BEST one or return 'ALL_INVALID' if none are executable.\n\nReturn JSON:\n{\n  \"choice\": \"<Candidate_A|Candidate_B|Candidate_C|ALL_INVALID>\",\n  \"reasoning\": \"<explanation>\",\n  \"issues\": [\"<list of problems if any>\"]\n}",
  "coreference_resolution_prompt": "You are a coreference resolution expert. Given a pronoun and surrounding context, identify what entity the pronoun refers to.\n\nReturn JSON with this structure:\n{\n  \"referent\": \"<entity name>\",\n  \"confidence\": \"<high|medium|low>\",\n  \"reasoning\": \"<brief explanation>\"\n}\n\nIf unable to resolve, return {\"referent\": null, \"confidence\": \"low\", \"reasoning\": \"insufficient context\"}",
  "sentence_split_prompt": "Split text into sentences. Return ONLY a JSON array.\n\nRules:\n- Preserve text exactly (no changes to words, punctuation, or capitalization)\n- Don't split on: Dr., Prof., Mr., Mrs., Ms., Inc., Ltd., Co., U.S., U.K., a.m., p.m., etc.\n- Don't split on decimals: $2.5 million, 3.14, 1.5 lbs.\n- Each sentence ends with . ! or ?\n- Return valid JSON array only\n\nExamples:\n\nInput: Dr. Chen left. He bought 1.5 lbs. of apples.\nOutput: [\"Dr. Chen left.\",\"He bought 1.5 lbs. of apples.\"]\n\nInput: The U.S. economy grew. Prof. Lee analyzed it.\nOutput: [\"The U.S. economy grew.\",\"Prof. Lee analyzed it.\"]"
}