# Upload Flow

## Overview

Document upload flow with hash-based deduplication and S3 presigned URL management.

## Flow Description

### Current Proposal Flow
1. **Client Request**: `POST /upload/request` with `{doc_name, doc_hash_256, file_size}`
2. **Hash Lookup**: Check if `doc_hash_256` exists in database
3. **New Document Path**:
   - Create row with unique `doc_hash_256`
   - Generate UUID as `doc_id` 
   - Create S3 presigned URL with expiry
   - Store metadata and timestamps
   - Return: `{doc_id, status, s3_presigned_url, s3_url_expires_at, trace_id}`
4. **Existing Document Path**:
   - Check URL expiry and regenerate if needed
   - Return existing document info
5. **Error Path**: Log to error table with trace correlation

## Database Schema

```sql
-- Main documents table for upload flow
CREATE TABLE documents (
    doc_id TEXT PRIMARY KEY,           -- UUID generated by server
    doc_hash_256 TEXT UNIQUE NOT NULL, -- Content hash (unique per user DB)
    original_filename TEXT NOT NULL,   -- User provided name
    file_size_bytes INTEGER,           -- Size validation
    s3_key TEXT,                       -- S3 object key
    upload_status TEXT NOT NULL,       -- ENUM status
    s3_url_generated_at INTEGER,       -- Unix timestamp when presigned URL was created
    s3_url_expires_at INTEGER,         -- Unix timestamp when URL expires
    created_at INTEGER DEFAULT (strftime('%s', 'now')),
    updated_at INTEGER DEFAULT (strftime('%s', 'now'))
);
```

## Implementation

### Two-Phase Database Strategy

```python
def handle_upload_request(doc_name, doc_hash_256, file_size):
    # Generate trace ID for end-to-end tracking
    trace_id = start_trace("upload_request")
    current_timestamp = int(time.time())
    
    try:
        # 1. Validate inputs
        if not validate_hash_format(doc_hash_256):
            log_error_with_trace(trace_id, "INVALID_HASH", 
                               f"Invalid hash format: {doc_hash_256}", "upload_request")
            return error_response("Invalid hash format", trace_id)
        
        # 2. Check for existing document
        existing = db.query("SELECT * FROM documents WHERE doc_hash_256=?", doc_hash_256)
        
        if existing:
            return handle_existing_document(existing, trace_id, current_timestamp)
        
        # 3. Create new document record FIRST (before S3 operations)
        doc_id = generate_uuid()
        s3_key = f"{doc_hash_256}/{doc_name}"
        
        # Phase 1: Insert row first - if S3 fails, at least we have the record
        db.insert("documents", {
            "doc_id": doc_id,
            "doc_hash_256": doc_hash_256,
            "original_filename": doc_name,
            "file_size_bytes": file_size,
            "s3_key": s3_key,
            "upload_status": "UPLOAD_PENDING",  # Not generated yet
            "created_at": current_timestamp,
            "updated_at": current_timestamp
        })
        
        # Update trace with doc_id
        db.update("request_traces", {"doc_id": doc_id}, {"trace_id": trace_id})
        
        # Phase 2: Generate presigned URL (after row creation)
        try:
            presigned_url = generate_s3_presigned_url(s3_key)
            expires_at = current_timestamp + (15 * 60)  # 15 minutes from now
            
            # Update with S3 URL info
            db.update("documents", {
                "upload_status": "UPLOAD_URL_GENERATED",
                "s3_url_generated_at": current_timestamp,
                "s3_url_expires_at": expires_at,
                "updated_at": current_timestamp
            }, {"doc_id": doc_id})
            
            complete_trace(trace_id, "SUCCESS", {
                "action": "new_document_created", 
                "doc_id": doc_id
            })
            
            return {
                "doc_id": doc_id,
                "status": "UPLOAD_URL_GENERATED",
                "s3_presigned_url": presigned_url,
                "s3_url_expires_at": expires_at,
                "trace_id": trace_id
            }
            
        except Exception as s3_error:
            # S3 failed, but row exists - update status
            db.update("documents", {
                "upload_status": "UPLOAD_FAILED",
                "updated_at": current_timestamp
            }, {"doc_id": doc_id})
            
            log_error_with_trace(trace_id, "S3_ERROR", 
                               f"S3 presigned URL generation failed: {str(s3_error)}", 
                               "upload_request")
            complete_trace(trace_id, "FAILED", {"error": "S3 URL generation failed"})
            
            return error_response("Failed to generate upload URL", trace_id)
        
    except Exception as e:
        log_error_with_trace(trace_id, "SYSTEM_ERROR", str(e), "upload_request", 
                           stack_trace=traceback.format_exc())
        complete_trace(trace_id, "FAILED", {"error": str(e)})
        return error_response("Internal server error", trace_id)

def handle_existing_document(existing, trace_id, current_timestamp):
    """Handle existing document with URL expiry logic"""
    if existing.upload_status == "UPLOAD_URL_GENERATED":
        # Check if URL has expired
        if current_timestamp > existing.s3_url_expires_at:
            # Generate new URL and update database
            presigned_url = generate_s3_presigned_url(existing.s3_key)
            expires_at = current_timestamp + (15 * 60)
            
            db.update("documents", {
                "s3_url_generated_at": current_timestamp,
                "s3_url_expires_at": expires_at,
                "updated_at": current_timestamp
            }, {"doc_id": existing.doc_id})
            
            complete_trace(trace_id, "SUCCESS", {"action": "url_regenerated"})
            return {
                "doc_id": existing.doc_id,
                "status": existing.upload_status,
                "s3_presigned_url": presigned_url,
                "s3_url_expires_at": expires_at,
                "trace_id": trace_id,
                "message": "New upload URL generated"
            }
        else:
            # URL still valid, return fresh URL anyway
            complete_trace(trace_id, "SUCCESS", {"action": "url_still_valid"})
            return {
                "doc_id": existing.doc_id,
                "status": existing.upload_status,
                "s3_presigned_url": generate_s3_presigned_url(existing.s3_key),
                "s3_url_expires_at": existing.s3_url_expires_at,
                "trace_id": trace_id,
                "message": "Using existing valid URL"
            }
    else:
        # Document exists and processed
        complete_trace(trace_id, "SUCCESS", {"action": "document_exists"})
        return {
            "doc_id": existing.doc_id,
            "status": existing.upload_status,
            "s3_presigned_url": None,
            "trace_id": trace_id,
            "message": "Document already processed"
        }
```

## Key Features

### 1. Hash-Based Deduplication
- Uses SHA-256 hash as unique identifier
- Prevents redundant processing of identical content
- File size validation for additional collision protection

### 2. Two-Phase Database Strategy
- **Phase 1**: Create document record first
- **Phase 2**: Generate S3 URL and update status
- Ensures row exists even if S3 operations fail

### 3. URL Expiry Management
- Unix timestamps for precise expiry tracking
- Automatic regeneration of expired URLs
- 15-minute expiry for security

### 4. Status Transitions
```
UPLOAD_PENDING → UPLOAD_URL_GENERATED → UPLOAD_COMPLETE
             ↘ UPLOAD_FAILED
```

## Edge Cases Handled

1. **Concurrent Requests** - Database constraints prevent duplicates
2. **S3 Failures** - Row exists with UPLOAD_FAILED status
3. **URL Expiry** - Automatic regeneration on subsequent requests
4. **Hash Validation** - Input validation before processing
5. **File Size Limits** - Configurable maximum file size

## Security Considerations

1. **Hash Validation** - Verify SHA-256 format
2. **File Size Limits** - Prevent abuse with large files
3. **URL Expiry** - Short-lived presigned URLs (15 minutes)
4. **Content Type** - Validate file types if needed
5. **Rate Limiting** - Prevent URL generation abuse

## Monitoring & Observability

- All operations tracked via `trace_id`
- Error correlation through error_logs table
- Performance metrics via duration tracking
- URL regeneration pattern analysis

## TODO Items

- [ ] Implement file content validation after upload
- [ ] Add support for multipart uploads for large files
- [ ] Implement cleanup job for expired/failed uploads
- [ ] Add metrics collection for upload success rates
- [ ] Implement rate limiting per user