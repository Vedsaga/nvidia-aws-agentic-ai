# Sentence Split Pipeline Implementation

## Overview
Implemented the sentence splitting mechanism as per `sentence_split_pipeline.md` diagram in `google_colab_cells.py`.

## Configuration
All settings in `SENTENCE_SPLIT_CONFIG`:
- `max_paragraph_tokens`: 3500 (threshold for single vs chunked - fits within output limit)
- `chunk_size_tokens`: 3000 (chunk size for long paragraphs)
- `overlap_tokens`: 200 (overlap between chunks - tokenizer-based)
- `max_retries`: 2 (self-correction attempts)
- `llm_max_output_tokens`: 4096 (must accommodate input text + JSON overhead)
- `fidelity_threshold`: 0.99 (99% similarity required for lenient fidelity check)

## Key Components

### 1. Main Pipeline: `_split_document_pipeline()`
- Splits document into paragraphs using `\n\n`
- Processes each paragraph based on token count
- Routes to single or chunked processing
- **Debug**: Shows paragraph count, token counts, routing decisions

### 2. Single Paragraph Processing: `_process_paragraph_single()`
- For paragraphs ≤ 6K tokens
- LLM proposes sentences as JSON array
- Aligns & extracts exact substrings from original
- Fidelity check: `join(sentences) == normalize(paragraph)`
- Self-correction on failure (configurable retries)
- Hybrid fallback if all retries fail
- **Debug**: `[P{idx}A{attempt}:Status]` format shows:
  - `NoProposal`: LLM didn't return valid JSON
  - `Proposed=N`: LLM proposed N sentences
  - `AlignFail`: Fuzzy matching failed
  - `Aligned=N`: Successfully aligned N sentences
  - `FidelityPass`: Exact match verified
  - `FidelityFail`: Mismatch detected
  - `AllRetriesFailed→Hybrid`: Using fallback

### 3. Chunked Processing: `_process_paragraph_chunked()`
- For paragraphs > 6K tokens
- Splits into 5K token chunks with 500-token overlap
- Processes each chunk independently
- Merges results with de-duplication (preserves first occurrence)
- **Debug**: Shows chunk count, per-chunk processing, merge count

### 4. Feedback Mechanism
- `_generate_alignment_feedback()`: Shows original vs proposed when alignment fails
- `_generate_fidelity_feedback()`: Shows mismatch position and context when fidelity fails
- Feedback is passed to LLM on retry attempts

### 5. Improved Robustness Features

#### Lenient Fidelity Check (`_verify_exact_fidelity`)
- Uses difflib similarity ratio instead of exact string match
- Normalizes: unicode (NFKC), whitespace, case
- Accepts 99%+ similarity (configurable threshold)
- Tolerates trivial formatting differences
- Still catches real hallucinations
- Shows `[FidelityRatio=0.98<0.99]` when threshold not met

#### Tokenizer-Based Chunking (`_process_paragraph_chunked`)
- Uses actual tokenizer for chunk boundaries
- Guaranteed not to exceed context window
- Works with multilingual text and emojis
- Overlap handled naturally in token space
- More accurate than character-based chunking

#### Cursor-Based Merge (`_merge_overlapping_chunks`)
- Prevents skipping repeated phrases
- Avoids false duplicates (e.g., multiple "Thank you.")
- Enforces forward progress with cursor
- Fuzzy fallback for small alignment shifts
- Preserves sentence order

### 6. Helper Methods
- `_count_tokens()`: Estimates token count (1 token ≈ 4 chars)
- `_llm_propose_sentences()`: LLM call with optional feedback
- `_merge_overlapping_chunks()`: De-duplicates overlapping sentences
- `_align_and_reconstruct()`: Fuzzy matching alignment (existing)
- `_verify_exact_fidelity()`: Whitespace-normalized verification (existing)
- `_hybrid_sentence_split()`: LLM + rule-based voting (existing)
- `_fallback_sentence_split()`: Safe regex splitter (existing)

## Token Clarification
- **Input tokens**: Text sent to LLM (prompt + user text)
- **Output tokens**: Text generated by LLM (response)
- **max_tokens parameter**: Limits OUTPUT tokens only (not input)
- Input and output are counted separately in API billing
- We increased `llm_max_output_tokens` to 4096 to handle longer sentence lists

## Flow Diagram Mapping

```
Input Document → Split Paragraphs → For Each Paragraph:
  ├─ Length ≤ 3.5K? → _process_paragraph_single()
  │   ├─ LLM Propose (with feedback if retry)
  │   ├─ Align → Fidelity Check
  │   ├─ Pass → Accept
  │   └─ Fail → Generate Feedback → Retry (max 2x) → Hybrid Fallback
  │
  └─ Length > 3.5K? → _process_paragraph_chunked()
      ├─ Split into 3K chunks + 300 overlap
      ├─ Process each chunk (with retries)
      └─ Merge & De-duplicate
```

## Token Management

### Why 3.5K Token Limit?
- Input text of 3.5K tokens needs ~3.5K+ output tokens for JSON array
- JSON overhead: quotes, commas, brackets add ~10-20% to output
- With 4096 max output tokens, 3.5K input leaves safe margin
- Larger paragraphs automatically chunked with overlap

### Exact Token Counting
- Uses actual tokenizer: `tokenizer.encode(text, add_special_tokens=False)`
- No more rough estimates (len/4) - 100% accurate
- Fallback to estimate only if tokenizer fails

### Retry Token Management
During retries, feedback is added to system prompt, increasing input tokens:
- **Base prompt**: ~400 tokens (sentence_split_prompt)
- **Text**: up to 3500 tokens
- **Feedback**: ~50-100 tokens (kept concise)
- **Total input**: ~4000 tokens max

**Dynamic adjustment in `_llm_propose_sentences()`:**
- Calculates: `total_input = base_prompt + text + feedback`
- If `total_input > 3500`, reduces output token limit proportionally
- Ensures: `input + output < model context window (8K)`
- Shows debug: `[TokenOverflow:Input=N]` when adjustment needed

**Feedback optimization:**
- Alignment feedback: <100 tokens (shows preview + count)
- Fidelity feedback: <100 tokens (shows mismatch position + minimal context)
- No verbose explanations, just actionable hints

## Debug Output Format
```
📦 Found 3 paragraph(s)
📝 P1/3 (1200 tokens)... (single) [P1A0:Proposed=5] [P1A0:Aligned=5] [P1A0:FidelityPass] ✅ 5 sentences
📝 P2/3 (8000 tokens)... (chunked) [P2:Chunks=2]
   C1/2 [P2.1A0:Proposed=12] [P2.1A0:Aligned=12] [P2.1A0:FidelityPass]
   C2/2 [P2.2A0:Proposed=10] [P2.2A0:Aligned=10] [P2.2A0:FidelityPass]
   [P2:Merged=20] ✅ 20 sentences
📝 P3/3 (500 tokens)... (single) [P3A0:Proposed=3] [P3A0:AlignFail]
   🔄 Retry 1/2 [P3A1:Proposed=3] [P3A1:Aligned=3] [P3A1:FidelityPass] ✅ 3 sentences
```

## Integration
- Modified `_load_documents()` to call `_split_document_pipeline()`
- Removed old chunking logic
- Maintains backward compatibility with processed file caching

## Testing
Run ingestion to test:
```python
graph.ingest_documents("./data")
```
