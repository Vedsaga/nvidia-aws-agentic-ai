#!/bin/bash

# End-to-End GSSR Test Script
# Tests: Upload â†’ Sentence Split â†’ GSSR KG Creation â†’ Embedding â†’ NetworkX â†’ Query â†’ Answer
# Adds post-run diagnostics for DynamoDB logging tables and sentence state.

set -e

echo "=========================================="
echo "End-to-End GSSR Pipeline Test"
echo "=========================================="
echo ""

# Basic dependency checks (curl is assumed on most systems, but jq is critical)
for tool in curl jq; do
    if ! command -v "$tool" >/dev/null 2>&1; then
        echo "âŒ Error: '$tool' is required but not installed."
        exit 1
    fi
done

# Load environment variables if available
if [ -f .env ]; then
    # shellcheck disable=SC2046
    export $(grep -v '^#' .env | xargs)
fi

API_URL="${APP_API_GATEWAY_URL}"
if [ -z "$API_URL" ]; then
    echo "âŒ Error: APP_API_GATEWAY_URL not found in environment (.env)"
    exit 1
fi

# Ensure API_URL ends with /
if [[ ! "$API_URL" =~ /$ ]]; then
    API_URL="${API_URL}/"
fi

LLM_CALL_LOG_TABLE="${LLM_CALL_LOG_TABLE:-LLMCallLog}"
LLM_CALL_PARSED_TABLE="${LLM_CALL_PARSED_TABLE:-LLMCallExtracts}"
SENTENCES_TABLE="${SENTENCES_TABLE:-Sentences}"
AWS_REGION="${AWS_REGION:-us-west-2}"
GSSR_LOG_DIR="${GSSR_LOG_DIR:-gssr-artifacts}"

AWS_COMMON_ARGS=(--region "$AWS_REGION")
if [ -n "${DYNAMODB_ENDPOINT_URL:-}" ]; then
    AWS_COMMON_ARGS+=(--endpoint-url "$DYNAMODB_ENDPOINT_URL")
fi

HAVE_AWS=false
if command -v aws >/dev/null 2>&1; then
    HAVE_AWS=true
else
    echo "âš ï¸ AWS CLI not found; DynamoDB inspection steps will be skipped."
fi

mkdir -p "$GSSR_LOG_DIR"

query_table_by_job() {
    local table=$1
    local index=$2
    local output=""

    if [ "$HAVE_AWS" != "true" ]; then
        return 1
    fi

    if [ -n "$index" ]; then
        if output=$(aws dynamodb query \
            --table-name "$table" \
            --index-name "$index" \
            --key-condition-expression "job_id = :jid" \
            --expression-attribute-values "{\":jid\":{\"S\":\"$JOB_ID\"}}" \
            "${AWS_COMMON_ARGS[@]}" \
            --output json 2>/tmp/gssr_query_err); then
            echo "$output"
            return 0
        fi
        echo "âš ï¸ Query on $table via index $index failed, falling back to scan." >&2
    fi

    if output=$(aws dynamodb scan \
        --table-name "$table" \
        --filter-expression "job_id = :jid" \
        --expression-attribute-values "{\":jid\":{\"S\":\"$JOB_ID\"}}" \
        "${AWS_COMMON_ARGS[@]}" \
        --output json 2>/tmp/gssr_query_err); then
        echo "$output"
        return 0
    fi

    return 1
}

echo "API URL: $API_URL"
echo ""

# Create test file with multiple sentences (unique content each run)
TEST_FILE="test-e2e-gssr.txt"
TIMESTAMP=$(date +%s)
RANDOM_ID=$((RANDOM % 10000))
echo "Dr. Elena Kowalski served as chief neuroscientist at the Berlin Institute from 2018 to 2023. She collaborated with Maria Santos on a groundbreaking study examining neuroplasticity. Test run: $TIMESTAMP-$RANDOM_ID." > "$TEST_FILE"

echo "âœ“ Created test file: $TEST_FILE"
cat "$TEST_FILE"
echo ""

# Step 1: Request upload URL
echo "=========================================="
echo "Step 1: Requesting Upload URL"
echo "=========================================="

UPLOAD_RESPONSE=$(curl -s -X POST "${API_URL}upload" \
  -H "Content-Type: application/json" \
  -d "{\"filename\": \"$TEST_FILE\"}")

echo "$UPLOAD_RESPONSE" | jq '.'

JOB_ID=$(echo "$UPLOAD_RESPONSE" | jq -r '.job_id')
PRE_SIGNED_URL=$(echo "$UPLOAD_RESPONSE" | jq -r '.pre_signed_url')

if [ -z "$JOB_ID" ] || [ "$JOB_ID" == "null" ]; then
    echo "âŒ Failed to get job_id"
    exit 1
fi

LOG_DIR="$GSSR_LOG_DIR/$JOB_ID"
mkdir -p "$LOG_DIR"

echo "" > "$LOG_DIR/run-metadata.txt"
{
    echo "JOB_ID=$JOB_ID"
    echo "TEST_FILE=$TEST_FILE"
    echo "API_URL=$API_URL"
    echo "LLM_CALL_LOG_TABLE=$LLM_CALL_LOG_TABLE"
    echo "LLM_CALL_PARSED_TABLE=$LLM_CALL_PARSED_TABLE"
    echo "SENTENCES_TABLE=$SENTENCES_TABLE"
    echo "AWS_REGION=$AWS_REGION"
    if [ -n "${DYNAMODB_ENDPOINT_URL:-}" ]; then
        echo "DYNAMODB_ENDPOINT_URL=$DYNAMODB_ENDPOINT_URL"
    fi
} >> "$LOG_DIR/run-metadata.txt"

echo ""
echo "âœ“ Job ID: $JOB_ID"
echo ""

# Step 2: Upload file to S3
echo "=========================================="
echo "Step 2: Uploading File to S3"
echo "=========================================="

if ! UPLOAD_OUTPUT=$(curl --fail --silent --show-error --upload-file "$TEST_FILE" "$PRE_SIGNED_URL" 2>&1); then
    echo "âŒ File upload failed"
    echo "$UPLOAD_OUTPUT"
    exit 1
fi

if [ -n "$UPLOAD_OUTPUT" ]; then
    echo "$UPLOAD_OUTPUT"
fi

echo "âœ“ File uploaded"
echo ""

# Step 3: Monitor processing status
echo "=========================================="
echo "Step 3: Monitoring KG Processing"
echo "=========================================="
echo "This will show GSSR flow in action..."
echo ""

MAX_CHECKS=60
CHECK_COUNT=0
PROCESSING_COMPLETE=false

while [ $CHECK_COUNT -lt $MAX_CHECKS ]; do
    CHECK_COUNT=$((CHECK_COUNT + 1))

    STATUS_RESPONSE=$(curl -s "${API_URL}status/${JOB_ID}")

    STATUS=$(echo "$STATUS_RESPONSE" | jq -r '.status')
    PROGRESS=$(echo "$STATUS_RESPONSE" | jq -r '.progress_percentage')
    TOTAL_SENTENCES=$(echo "$STATUS_RESPONSE" | jq -r '.total_sentences')
    COMPLETED_SENTENCES=$(echo "$STATUS_RESPONSE" | jq -r '.completed_sentences')
    LLM_CALLS=$(echo "$STATUS_RESPONSE" | jq -r '.llm_calls_made')

    echo "Check $CHECK_COUNT: Status=$STATUS, Progress=$PROGRESS%, Sentences=$COMPLETED_SENTENCES/$TOTAL_SENTENCES, LLM Calls=$LLM_CALLS"

    if [ "$STATUS" == "completed" ]; then
        PROCESSING_COMPLETE=true
        echo ""
        echo "âœ“ Processing complete!"
        echo "$STATUS_RESPONSE" | jq '.' > "$LOG_DIR/status-final.json"
        echo "$STATUS_RESPONSE" | jq '.'
        break
    elif [ "$STATUS" == "failed" ]; then
        echo ""
        echo "âŒ Processing failed!"
        echo "$STATUS_RESPONSE" | jq '.'
        exit 1
    fi

    sleep 5
done

if [ "$PROCESSING_COMPLETE" = false ]; then
    echo ""
    echo "âš ï¸  Processing timeout after $MAX_CHECKS checks"
    exit 1
fi

echo ""

# Step 4: Collect LLM logs and parsed extracts
echo "=========================================="
echo "Step 4: Collecting LLM Gateway Logs"
echo "=========================================="

HAVE_LLM_LOGS=false
HAVE_PARSED_LOGS=false
LLM_LOG_SUMMARY_FILE=""
LLM_PARSED_SUMMARY_FILE=""

if [ "$HAVE_AWS" = true ]; then
    if LLM_LOG_RAW=$(query_table_by_job "$LLM_CALL_LOG_TABLE" "ByJobId"); then
        echo "$LLM_LOG_RAW" > "$LOG_DIR/llm-call-log-raw.json"
        if echo "$LLM_LOG_RAW" | jq -e '.Items | length > 0' >/dev/null 2>&1; then
            HAVE_LLM_LOGS=true
            LLM_LOG_SUMMARY_FILE="$LOG_DIR/llm-call-log-summary.json"
            echo "$LLM_LOG_RAW" | jq '[.Items[] | {
                call_id: .call_id.S,
                timestamp: (.timestamp.N | tonumber),
                job_id: (.job_id.S // ""),
                sentence_hash: (.sentence_hash.S // ""),
                stage: (.pipeline_stage.S // .stage.S // ""),
                status: (.status.S // ""),
                prompt: (.prompt_template.S // ""),
                attempt: ((.attempt_number.N // "0") | tonumber),
                generation: ((.generation_index.N // "0") | tonumber),
                temperature: ((.temperature.N // "0") | tonumber),
                latency_ms: ((.latency_ms.N // "0") | tonumber),
                has_extracted_json: ((.extracted_json.S // "") | length > 0),
                reasoning_preview: ((.extracted_reasoning.S // "") | .[0:120])
            }] | sort_by(.timestamp)' > "$LLM_LOG_SUMMARY_FILE"

            echo "âœ“ Retrieved $(jq 'length' "$LLM_LOG_SUMMARY_FILE") LLM gateway log entries"
            echo ""
            echo "LLM Calls by pipeline stage:"
            jq -r '.[] | .stage' "$LLM_LOG_SUMMARY_FILE" | sort | uniq -c
            echo ""
            echo "Prompt usage distribution:"
            jq -r '.[] | .prompt' "$LLM_LOG_SUMMARY_FILE" | sort | uniq -c
            echo ""

            D1_COUNT=$(jq '[.[] | select(.stage | contains("D1"))] | length' "$LLM_LOG_SUMMARY_FILE")
            D2A_COUNT=$(jq '[.[] | select(.stage | contains("D2a"))] | length' "$LLM_LOG_SUMMARY_FILE")
            D2B_COUNT=$(jq '[.[] | select(.stage | contains("D2b"))] | length' "$LLM_LOG_SUMMARY_FILE")
            SCORER_COUNT=$(jq '[.[] | select(.prompt == "scorer.txt")] | length' "$LLM_LOG_SUMMARY_FILE")
            CORRECTION_COUNT=$(jq '[.[] | select(.prompt == "correction_prompt.txt")] | length' "$LLM_LOG_SUMMARY_FILE")

            [[ $D1_COUNT -gt 0 ]] && echo "âœ“ Entities stage (D1) calls: $D1_COUNT" || echo "âŒ No D1 stage calls found"
            [[ $D2A_COUNT -gt 0 ]] && echo "âœ“ Kriya stage (D2a) calls: $D2A_COUNT" || echo "âŒ No D2a stage calls found"
            [[ $D2B_COUNT -gt 0 ]] && echo "âœ“ Event/Karaka stage (D2b) calls: $D2B_COUNT" || echo "âŒ No D2b stage calls found"
            [[ $SCORER_COUNT -gt 0 ]] && echo "âœ“ Scorer prompt invocations: $SCORER_COUNT" || echo "âš ï¸ Scorer prompt not invoked"
            [[ $CORRECTION_COUNT -gt 0 ]] && echo "â„¹ï¸ Correction prompts invoked: $CORRECTION_COUNT" || echo "â„¹ï¸ No correction prompts needed"

            echo ""
            echo "LLM call timeline (first 20 entries):"
            jq -r '.[] | "\(.timestamp) | \(.stage) | Attempt \(.attempt) Gen \(.generation) | Temp \(.temperature) | \(.status)"' "$LLM_LOG_SUMMARY_FILE" | head -20
            echo ""
        else
            echo "âš ï¸ No entries found in $LLM_CALL_LOG_TABLE for job $JOB_ID"
        fi
    else
        echo "âš ï¸ Unable to query $LLM_CALL_LOG_TABLE"
    fi

    if PARSED_LOG_RAW=$(query_table_by_job "$LLM_CALL_PARSED_TABLE" "ByJobId"); then
        echo "$PARSED_LOG_RAW" > "$LOG_DIR/llm-call-extracts-raw.json"
        if echo "$PARSED_LOG_RAW" | jq -e '.Items | length > 0' >/dev/null 2>&1; then
            HAVE_PARSED_LOGS=true
            LLM_PARSED_SUMMARY_FILE="$LOG_DIR/llm-call-extracts-summary.json"
            echo "$PARSED_LOG_RAW" | jq '[.Items[] | {
                call_id: .call_id.S,
                sentence_hash: (.sentence_hash.S // ""),
                stage: (.pipeline_stage.S // ""),
                attempt: ((.attempt_number.N // "0") | tonumber),
                generation: ((.generation_index.N // "0") | tonumber),
                reasoning_preview: ((.extracted_reasoning.S // "") | .[0:160]),
                extracted_json: (.extracted_json.S // "")
            }] | sort_by(.call_id)' > "$LLM_PARSED_SUMMARY_FILE"

            echo "âœ“ Captured $(jq 'length' "$LLM_PARSED_SUMMARY_FILE") parsed JSON extracts"
            echo ""

            # Show sample parsed payload
            SAMPLE_JSON=$(jq -r 'map(select(.extracted_json != "")) | .[0].extracted_json // ""' "$LLM_PARSED_SUMMARY_FILE")
            if [ -n "$SAMPLE_JSON" ]; then
                echo "Sample parsed JSON from first entry:"
                if echo "$SAMPLE_JSON" | jq '.' >/dev/null 2>&1; then
                    echo "$SAMPLE_JSON" | jq '.'
                else
                    echo "$SAMPLE_JSON"
                fi
                echo ""
            fi
        else
            echo "âš ï¸ No parsed extracts found in $LLM_CALL_PARSED_TABLE for job $JOB_ID"
        fi
    else
        echo "âš ï¸ Unable to query $LLM_CALL_PARSED_TABLE"
    fi
else
    echo "âš ï¸ Skipped log inspection (AWS CLI unavailable)"
fi

# Step 5: Inspect sentence table snapshot
echo "=========================================="
echo "Step 5: Inspecting Sentence Table"
echo "=========================================="

if [ "$HAVE_AWS" = true ]; then
    if SENTENCE_RAW=$(query_table_by_job "$SENTENCES_TABLE" "ByJobId"); then
        echo "$SENTENCE_RAW" > "$LOG_DIR/sentences-raw.json"
        if echo "$SENTENCE_RAW" | jq -e '.Items | length > 0' >/dev/null 2>&1; then
            SENTENCE_SUMMARY_FILE="$LOG_DIR/sentences-summary.json"
            echo "$SENTENCE_RAW" | jq '[.Items[] | {
                sentence_hash: .sentence_hash.S,
                status: (.status.S // ""),
                best_score: (.best_score.N // "N/A"),
                needs_review: (.needs_review.BOOL // false),
                d1_attempts: (.d1_attempts.N // "0"),
                d2a_attempts: (.d2a_attempts.N // "0"),
                d2b_attempts: (.d2b_attempts.N // "0"),
                failure_reason: (.failure_reason.S // ""),
                original_sentence: (.original_sentence.S // .text.S // "")
            }]' > "$SENTENCE_SUMMARY_FILE"

            SENTENCE_COUNT=$(jq 'length' "$SENTENCE_SUMMARY_FILE")
            echo "âœ“ Sentences processed: $SENTENCE_COUNT"
            echo ""
            jq -r '.[] | "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nSentence: \(.original_sentence)\nHash: \(.sentence_hash)\nStatus: \(.status) | Best Score: \(.best_score) | Needs Review: \(.needs_review)\nAttempts â†’ D1: \(.d1_attempts) | D2a: \(.d2a_attempts) | D2b: \(.d2b_attempts)\n"' "$SENTENCE_SUMMARY_FILE" | head -6

            echo "Summary stats:"
            jq -r '.[] | .status' "$SENTENCE_SUMMARY_FILE" | sort | uniq -c
            echo ""
        else
            echo "âš ï¸ No sentence rows found for job $JOB_ID"
        fi
    else
        echo "âš ï¸ Unable to query $SENTENCES_TABLE"
    fi
else
    echo "âš ï¸ Skipped sentence inspection (AWS CLI unavailable)"
fi

# Step 6: Test Query - Sentence Retrieval
echo "=========================================="
echo "Step 6: Testing Query - Sentence Retrieval"
echo "=========================================="

QUESTION="Who worked at the Berlin Institute?"
echo "Question: $QUESTION"
echo ""

# Note: Using POST /query endpoint (not /query/submit)
QUERY_RESPONSE=$(curl -s -X POST "${API_URL}query" \
  -H "Content-Type: application/json" \
  -d "{\"query\": \"$QUESTION\"}")

echo "$QUERY_RESPONSE" | jq '.'

# Check if query succeeded
HAS_ANSWER=$(echo "$QUERY_RESPONSE" | jq -r '.answer // empty')
HAS_ERROR=$(echo "$QUERY_RESPONSE" | jq -r '.error // empty')
HAS_MESSAGE=$(echo "$QUERY_RESPONSE" | jq -r '.message // empty')

if [ -n "$HAS_ERROR" ]; then
    echo "âŒ Query failed with error field!"
    echo "$QUERY_RESPONSE" | jq '.'
    exit 1
fi

if [ -n "$HAS_MESSAGE" ] && [[ "$HAS_MESSAGE" == *"Internal server error"* ]]; then
    echo "âŒ Query failed with internal server error!"
    echo "$QUERY_RESPONSE" | jq '.'
    echo ""
    echo "Checking Lambda logs for errors..."
    if [ "$HAVE_AWS" = true ]; then
        aws logs tail /aws/lambda/ServerlessStack-SynthesizeAnswer450E341F-0If5YNYsNVD4 --since 2m --format short "${AWS_COMMON_ARGS[@]}" 2>/dev/null || echo "Could not retrieve logs"
    fi
    exit 1
fi

if [ -z "$HAS_ANSWER" ]; then
    echo "âš ï¸  Query response missing 'answer' field"
    echo "$QUERY_RESPONSE" | jq '.'
fi

echo ""
echo "âœ“ Query completed (synchronous response)"
echo ""

# Step 7: Display final answer with context
echo "=========================================="
echo "Step 7: Final Answer with Context"
echo "=========================================="
echo ""

echo "$QUERY_RESPONSE" | jq '.' > "$LOG_DIR/query-final.json"
echo "$QUERY_RESPONSE" | jq '.'

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ANSWER:"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "$QUERY_RESPONSE" | jq -r '.answer'
echo ""

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "SUPPORTING EVIDENCE:"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

REFERENCES=$(echo "$QUERY_RESPONSE" | jq -c '.references // [] | .[]')

if [ -n "$REFERENCES" ]; then
    echo "$REFERENCES" | while IFS= read -r ref; do
        SENTENCE=$(echo "$ref" | jq -r '.sentence_text')
        SENT_HASH=$(echo "$ref" | jq -r '.sentence_hash')

        echo ""
        echo "ğŸ“ Sentence: $SENTENCE"
        echo "   Hash: $SENT_HASH"
        echo ""
        echo "   ğŸ”— Knowledge Graph:"

    echo "   Nodes:"
    echo "$ref" | jq -r '.kg_snippet.nodes // [] | .[] | "      - \(.id) (\(.node_type))"'

        echo ""
    echo "   Edges:"
    echo "$ref" | jq -r '.kg_snippet.edges // [] | .[] | "      - \(.source) â†’ \(.target) [\(.label)]"'

        echo ""
        echo "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    done
else
    echo "No references found"
fi

echo ""

# Step 8: GSSR quality metrics and temperature/attempt distribution
echo "=========================================="
echo "Step 8: GSSR Quality Metrics"
echo "=========================================="
echo ""

if [ "$HAVE_LLM_LOGS" = true ]; then
    echo "Prompt usage summary:"
    jq -r '.[] | .prompt' "$LLM_LOG_SUMMARY_FILE" | sort | uniq -c
    echo ""

    echo "Temperature distribution:"
    jq -r '.[] | (.temperature | tostring)' "$LLM_LOG_SUMMARY_FILE" | sort | uniq -c
    echo ""

    echo "Attempt distribution by stage:"
    jq -r '.[] | "\(.stage): Attempt \(.attempt)"' "$LLM_LOG_SUMMARY_FILE" | sort | uniq -c
    echo ""

    echo "Entries with parsed JSON captured: $(jq '[.[] | select(.has_extracted_json == true)] | length' "$LLM_LOG_SUMMARY_FILE")"
    if [ "$HAVE_PARSED_LOGS" = true ]; then
        echo "Parsed extracts recorded: $(jq 'length' "$LLM_PARSED_SUMMARY_FILE")"
    fi
    echo ""
else
    echo "âš ï¸ Skipping GSSR metrics because LLM logs were not collected"
fi

# Cleanup test artifact
rm -f "$TEST_FILE"

echo "=========================================="
echo "âœ… End-to-End Test Complete!"
echo "=========================================="
echo ""
echo "Summary:"
echo "1. âœ“ File uploaded and processed"
echo "2. âœ“ Sentences split and hashed"
echo "3. âœ“ GSSR flow executed (3x generation, fidelity, consensus, scoring)"
echo "4. âœ“ Embeddings created and stored"
echo "5. âœ“ Knowledge graph built in NetworkX"
echo "6. âœ“ Query submitted and processed"
echo "7. âœ“ Relevant sentences retrieved via embedding"
echo "8. âœ“ Graph context fetched from NetworkX"
echo "9. âœ“ Answer synthesized by LLM"
echo "10. âœ“ DynamoDB logs captured in $LOG_DIR for audit"
echo ""
echo "ğŸ‰ All pipeline stages working correctly!"
