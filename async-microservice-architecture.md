# Async Microservice Architecture Design

## Core Principles

1. **One-Way API Execution** - Each API call either succeeds or fails, no retries
2. **Status-Driven Flow** - Every operation updates job status in SQLite
3. **Client-Controlled Execution** - Client triggers each step independently
4. **Failure Transparency** - All failure reasons captured and exposed
5. **Per-User Scaling** - SQLite enables user-specific databases

## Global Status Enum

```python
class JobStatus(Enum):
    # Upload Flow
    UPLOAD_PENDING = "upload_pending"
    UPLOAD_COMPLETE = "upload_complete" 
    UPLOAD_FAILED = "upload_failed"
    
    # Document Processing
    DOC_VALIDATION_PENDING = "doc_validation_pending"
    DOC_VALIDATION_COMPLETE = "doc_validation_complete"
    DOC_VALIDATION_FAILED = "doc_validation_failed"
    
    DOC_SANITIZE_PENDING = "doc_sanitize_pending"
    DOC_SANITIZE_COMPLETE = "doc_sanitize_complete"
    DOC_SANITIZE_FAILED = "doc_sanitize_failed"
    
    SENTENCE_SPLIT_PENDING = "sentence_split_pending"
    SENTENCE_SPLIT_COMPLETE = "sentence_split_complete"
    SENTENCE_SPLIT_FAILED = "sentence_split_failed"
    
    # KG Processing (per sentence)
    ENTITIES_PENDING = "entities_pending"
    ENTITIES_COMPLETE = "entities_complete"
    ENTITIES_FAILED = "entities_failed"
    
    KRIYA_PENDING = "kriya_pending"
    KRIYA_COMPLETE = "kriya_complete"
    KRIYA_FAILED = "kriya_failed"
    
    EVENTS_PENDING = "events_pending"
    EVENTS_COMPLETE = "events_complete"
    EVENTS_FAILED = "events_failed"
    
    RELATIONS_PENDING = "relations_pending"
    RELATIONS_COMPLETE = "relations_complete"
    RELATIONS_FAILED = "relations_failed"
    
    GRAPH_PENDING = "graph_pending"
    GRAPH_COMPLETE = "graph_complete"
    GRAPH_FAILED = "graph_failed"
    
    # Overall Job Status
    JOB_COMPLETE = "job_complete"
    JOB_FAILED = "job_failed"
```
## Upload Flow Analysis

### Current Proposal Flow
1. **Client Request**: `POST /upload/request` with `{doc_name, doc_hash_256}`
2. **Hash Lookup**: Check if `doc_hash_256` exists in database
3. **New Document Path**:
   - Create row with unique `doc_hash_256`
   - Generate UUID as `doc_id` 
   - Create S3 presigned URL with expiry
   - Store: `doc_hash, raw_filename, created_at, updated_at, upload_status, s3_url, s3_url_created_at, s3_url_expiry, doc_id`
   - Return: `{doc_id, status, created_at, updated_at, s3_presigned_url, s3_url_expiry}`
4. **Existing Document Path**:
   - Return same response format but `s3_presigned_url = null`
5. **Error Path**: Log to error table, no status to update

### Analysis & Improvements

#### Strengths
- **Deduplication** prevents redundant processing
- **Hash-based identity** ensures content uniqueness
- **Atomic operation** - single database transaction
- **Clear separation** between new vs existing documents

#### Potential Issues & Improvements

1. **Hash Collision Risk**
   - SHA-256 is strong but consider adding file size validation
   - Store `file_size_bytes` alongside hash for additional verification

2. **S3 URL Management**
   - Current: Store URL in database (potential stale data)
   - Better: Generate presigned URL on-demand (always fresh)
   - Store only S3 key pattern, generate URL when needed

3. **Upload Status Granularity**
   ```
   UPLOAD_URL_GENERATED → UPLOAD_IN_PROGRESS → UPLOAD_COMPLETE → UPLOAD_FAILED
   ```

4. **Filename Handling**
   - Multiple users might upload same content with different names
   - Store `original_filename` but use `doc_hash` for S3 key
   - S3 key pattern: `{user_id}/{doc_hash}/{original_filename}`

5. **User Context**
   - Add `user_id` to track ownership
   - Same document hash can exist for different users

6. **Expiry Management**
   - Presigned URLs expire (typically 1 hour)
   - Need mechanism to regenerate expired URLs
   - Consider shorter expiry (15 mins) with refresh endpoint

### Refined Upload Flow

#### Database Schema (Per-User SQLite)
```sql
-- Main documents table for upload flow
CREATE TABLE documents (
    doc_id TEXT PRIMARY KEY,           -- UUID generated by server
    doc_hash_256 TEXT UNIQUE NOT NULL, -- Content hash (unique per user DB)
    original_filename TEXT NOT NULL,   -- User provided name
    file_size_bytes INTEGER,           -- Size validation
    s3_key TEXT,                       -- S3 object key
    upload_status TEXT NOT NULL,       -- ENUM status
    s3_url_generated_at INTEGER,       -- Unix timestamp when presigned URL was created
    s3_url_expires_at INTEGER,         -- Unix timestamp when URL expires
    created_at INTEGER DEFAULT (strftime('%s', 'now')),
    updated_at INTEGER DEFAULT (strftime('%s', 'now'))
);
```

#### Execution Logic with Tracing
```python
def handle_upload_request(doc_name, doc_hash_256, file_size):
    # Generate trace ID for end-to-end tracking
    trace_id = generate_uuid()
    current_timestamp = int(time.time())
    
    try:
        # Start request trace
        db.insert("request_traces", {
            "trace_id": trace_id,
            "operation": "upload_request",
            "status": "IN_PROGRESS",
            "start_time": current_timestamp,
            "metadata": json.dumps({
                "doc_name": doc_name,
                "doc_hash_256": doc_hash_256,
                "file_size": file_size
            })
        })
        
        # 1. Validate inputs
        if not validate_hash_format(doc_hash_256):
            log_error_with_trace(trace_id, "INVALID_HASH", f"Invalid hash format: {doc_hash_256}")
            return error_response("Invalid hash format", trace_id)
        
        # 2. Check for existing document
        existing = db.query("SELECT * FROM documents WHERE doc_hash_256=?", doc_hash_256)
        
        if existing:
            # Check if URL needs regeneration
            if existing.upload_status == "UPLOAD_URL_GENERATED":
                # Check if URL has expired
                if current_timestamp > existing.s3_url_expires_at:
                    # Generate new URL and update database
                    presigned_url = generate_s3_presigned_url(existing.s3_key)
                    expires_at = current_timestamp + (15 * 60)  # 15 minutes from now
                    
                    db.update("documents", {
                        "s3_url_generated_at": current_timestamp,
                        "s3_url_expires_at": expires_at,
                        "updated_at": current_timestamp
                    }, {"doc_id": existing.doc_id})
                    
                    complete_trace(trace_id, "SUCCESS", {"action": "url_regenerated"})
                    return {
                        "doc_id": existing.doc_id,
                        "status": existing.upload_status,
                        "s3_presigned_url": presigned_url,
                        "s3_url_expires_at": expires_at,
                        "trace_id": trace_id,
                        "message": "New upload URL generated"
                    }
                else:
                    # URL still valid, return existing info
                    complete_trace(trace_id, "SUCCESS", {"action": "url_still_valid"})
                    return {
                        "doc_id": existing.doc_id,
                        "status": existing.upload_status,
                        "s3_presigned_url": generate_s3_presigned_url(existing.s3_key),
                        "s3_url_expires_at": existing.s3_url_expires_at,
                        "trace_id": trace_id,
                        "message": "Using existing valid URL"
                    }
            else:
                # Document exists and processed
                complete_trace(trace_id, "SUCCESS", {"action": "document_exists"})
                return {
                    "doc_id": existing.doc_id,
                    "status": existing.upload_status,
                    "s3_presigned_url": None,
                    "trace_id": trace_id,
                    "message": "Document already processed"
                }
        
        # 3. Create new document record FIRST (before S3 operations)
        doc_id = generate_uuid()
        s3_key = f"{doc_hash_256}/{doc_name}"
        
        # Insert row first - if S3 fails, at least we have the record
        db.insert("documents", {
            "doc_id": doc_id,
            "doc_hash_256": doc_hash_256,
            "original_filename": doc_name,
            "file_size_bytes": file_size,
            "s3_key": s3_key,
            "upload_status": "UPLOAD_PENDING",  # Not generated yet
            "created_at": current_timestamp,
            "updated_at": current_timestamp
        })
        
        # Update trace with doc_id
        db.update("request_traces", {"doc_id": doc_id}, {"trace_id": trace_id})
        
        # 4. Generate presigned URL (after row creation)
        try:
            presigned_url = generate_s3_presigned_url(s3_key)
            expires_at = current_timestamp + (15 * 60)  # 15 minutes from now
            
            # Update with S3 URL info
            db.update("documents", {
                "upload_status": "UPLOAD_URL_GENERATED",
                "s3_url_generated_at": current_timestamp,
                "s3_url_expires_at": expires_at,
                "updated_at": current_timestamp
            }, {"doc_id": doc_id})
            
            complete_trace(trace_id, "SUCCESS", {"action": "new_document_created", "doc_id": doc_id})
            
            return {
                "doc_id": doc_id,
                "status": "UPLOAD_URL_GENERATED",
                "s3_presigned_url": presigned_url,
                "s3_url_expires_at": expires_at,
                "trace_id": trace_id
            }
            
        except Exception as s3_error:
            # S3 failed, but row exists - update status
            db.update("documents", {
                "upload_status": "UPLOAD_FAILED",
                "updated_at": current_timestamp
            }, {"doc_id": doc_id})
            
            log_error_with_trace(trace_id, "S3_ERROR", f"S3 presigned URL generation failed: {str(s3_error)}")
            complete_trace(trace_id, "FAILED", {"error": "S3 URL generation failed"})
            
            return error_response("Failed to generate upload URL", trace_id)
        
    except Exception as e:
        log_error_with_trace(trace_id, "SYSTEM_ERROR", str(e), stack_trace=traceback.format_exc())
        complete_trace(trace_id, "FAILED", {"error": str(e)})
        return error_response("Internal server error", trace_id)

# Helper functions use application-wide error tracking (see Error Handling section)
```

### Additional Considerations

#### Edge Cases
1. **Concurrent Requests**: Same user, same hash - use database constraints to handle
2. **Partial Uploads**: S3 multipart uploads that fail midway
3. **Hash Mismatch**: Client provides wrong hash for file content
4. **File Size Limits**: Enforce maximum file size (e.g., 100MB)

#### Security Concerns
1. **Hash Validation**: Verify client-provided hash matches actual file
2. **User Authentication**: Ensure user_id is authenticated
3. **Rate Limiting**: Prevent abuse of presigned URL generation
4. **Content Type**: Validate file types (only text documents)

#### Monitoring & Observability
1. **Metrics**: Track upload success/failure rates
2. **Alerts**: Monitor for unusual hash collision patterns
3. **Cleanup**: Remove expired/failed upload records
4. **Storage**: Monitor S3 storage costs per user

This approach provides robust deduplication while maintaining user isolation and avoiding stale URL storage issues.

---
## Error Handling & Tracing System

### Global Error Tracking Strategy

The `error_logs` and `request_traces` tables are designed to be **application-wide**, supporting all user flows:

```sql
-- Application-wide error tracking
CREATE TABLE error_logs (
    error_id TEXT PRIMARY KEY,
    trace_id TEXT NOT NULL,            -- Links to request_traces
    operation TEXT NOT NULL,           -- upload_request, validate_doc, extract_entities, etc.
    error_code TEXT,                   -- Structured codes: INVALID_HASH, S3_ERROR, LLM_TIMEOUT
    error_message TEXT NOT NULL,
    stack_trace TEXT,
    context_data TEXT,                 -- JSON blob with operation-specific data
    created_at INTEGER DEFAULT (strftime('%s', 'now')),
    FOREIGN KEY (trace_id) REFERENCES request_traces(trace_id)
);

-- Application-wide request tracing
CREATE TABLE request_traces (
    trace_id TEXT PRIMARY KEY,         -- UUID for entire request flow
    parent_trace_id TEXT,              -- For nested operations (sentence processing)
    operation TEXT NOT NULL,           -- Operation name across all microservices
    doc_id TEXT,                       -- Associated document (if applicable)
    sentence_hash TEXT,                -- Associated sentence (if applicable)  
    status TEXT NOT NULL,              -- SUCCESS/FAILED/IN_PROGRESS
    start_time INTEGER DEFAULT (strftime('%s', 'now')),
    end_time INTEGER,
    duration_ms INTEGER,
    metadata TEXT,                     -- JSON blob with operation details
    FOREIGN KEY (doc_id) REFERENCES documents(doc_id)
);
```

### Operation Examples Across App

```python
# Upload flow
trace_id = start_trace("upload_request", doc_id=doc_id)

# Document processing  
trace_id = start_trace("validate_document", doc_id=doc_id, parent_trace_id=upload_trace_id)
trace_id = start_trace("split_sentences", doc_id=doc_id, parent_trace_id=upload_trace_id)

# Sentence processing (parallel)
for sentence in sentences:
    sentence_trace_id = start_trace("extract_entities", 
                                   doc_id=doc_id, 
                                   sentence_hash=sentence.hash,
                                   parent_trace_id=split_trace_id)
```

### Error Code Standardization

```python
class ErrorCodes:
    # Input validation
    INVALID_HASH = "INVALID_HASH"
    INVALID_FILE_SIZE = "INVALID_FILE_SIZE"
    MISSING_REQUIRED_FIELD = "MISSING_REQUIRED_FIELD"
    
    # Infrastructure
    S3_ERROR = "S3_ERROR"
    DATABASE_ERROR = "DATABASE_ERROR"
    LLM_TIMEOUT = "LLM_TIMEOUT"
    LLM_RATE_LIMIT = "LLM_RATE_LIMIT"
    
    # Business logic
    DOCUMENT_NOT_FOUND = "DOCUMENT_NOT_FOUND"
    SENTENCE_PROCESSING_FAILED = "SENTENCE_PROCESSING_FAILED"
    GSSR_MAX_ATTEMPTS = "GSSR_MAX_ATTEMPTS"
    
    # System
    SYSTEM_ERROR = "SYSTEM_ERROR"
    TIMEOUT = "TIMEOUT"
```

### Helper Functions (Used by All Flows)

```python
def log_error_with_trace(trace_id, error_code, message, operation, stack_trace=None):
    db.insert("error_logs", {
        "error_id": generate_uuid(),
        "trace_id": trace_id,
        "operation": operation,
        "error_code": error_code,
        "error_message": message,
        "stack_trace": stack_trace,
        "context_data": json.dumps({"trace_id": trace_id})
    })

def complete_trace(trace_id, status, metadata=None):
    end_time = int(time.time())
    start_time = db.query("SELECT start_time FROM request_traces WHERE trace_id=?", trace_id)[0].start_time
    duration_ms = (end_time - start_time) * 1000
    
    db.update("request_traces", {
        "status": status,
        "end_time": end_time,
        "duration_ms": duration_ms,
        "metadata": json.dumps(metadata) if metadata else None
    }, {"trace_id": trace_id})

def start_trace(operation, doc_id=None, sentence_hash=None, parent_trace_id=None):
    trace_id = generate_uuid()
    db.insert("request_traces", {
        "trace_id": trace_id,
        "parent_trace_id": parent_trace_id,
        "operation": operation,
        "doc_id": doc_id,
        "sentence_hash": sentence_hash,
        "status": "IN_PROGRESS",
        "start_time": int(time.time())
    })
    return trace_id
```

### Industry Standard Practices

1. **Distributed Tracing**
   - Each request gets unique `trace_id` (UUID)
   - All operations within request use same `trace_id`
   - Nested operations get `parent_trace_id` for hierarchy
   - Standard: OpenTelemetry, Jaeger, Zipkin

2. **Structured Logging**
   ```python
   # Every log entry includes trace context
   logger.info("Document validation started", extra={
       "trace_id": trace_id,
       "doc_id": doc_id,
       "operation": "validate_document"
   })
   ```

3. **Request Correlation**
   - Client includes `X-Request-ID` header
   - Server generates `trace_id` if not provided
   - All downstream calls propagate trace context

### Cross-Operation Debugging

```sql
-- Find all operations for a document
SELECT rt.operation, rt.status, rt.duration_ms, el.error_code
FROM request_traces rt
LEFT JOIN error_logs el ON rt.trace_id = el.trace_id  
WHERE rt.doc_id = ?
ORDER BY rt.start_time;

-- Find all failed sentence processing
SELECT rt.sentence_hash, rt.operation, el.error_message
FROM request_traces rt
JOIN error_logs el ON rt.trace_id = el.trace_id
WHERE rt.operation LIKE '%extract_%' AND rt.status = 'FAILED';

-- Performance analysis across operations
SELECT operation, 
       AVG(duration_ms) as avg_duration,
       COUNT(*) as total_calls,
       SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failures
FROM request_traces 
GROUP BY operation;

-- Track URL regeneration patterns  
SELECT doc_hash_256, COUNT(*) as regeneration_count
FROM request_traces rt
JOIN documents d ON rt.doc_id = d.doc_id
WHERE rt.operation = 'upload_request' AND rt.metadata LIKE '%url_regenerated%'
GROUP BY doc_hash_256;

-- End-to-end request flow
SELECT operation, status, duration_ms, start_time
FROM request_traces 
WHERE trace_id = ? 
ORDER BY start_time;
```

This unified approach gives you complete visibility across the entire application pipeline, not just individual microservices.