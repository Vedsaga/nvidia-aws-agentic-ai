# Configuration for Karaka Knowledge Graph System
# All configurable values extracted from google_colab_cells.py

# Model Configuration
models:
  llm:
    model_id: "nvidia/Llama-3.1-Nemotron-Nano-8B-v1"
    dtype_primary: "bfloat16"
    dtype_fallback: "float16"
    device_map: "auto"
    trust_remote_code: true
  
  embedding:
    model_id: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    device: "cuda"  # or "cpu"
    trust_remote_code: true
    dimension: 2048

# Entity Resolution
entity_resolution:
  similarity_threshold: 0.85

# File Paths
file_paths:
  db_file: "karaka_graph.json"
  graph_viz_file: "karaka_graph.gexf"
  prompts_yaml_file: "prompts.yaml"  # All prompts in YAML format
  docs_folder: "./data"

# Graph Schema
graph_schema:
  node_types:
    - "Kriya"
    - "Entity"
    - "Document"
  
  edge_relations:
    - "HAS_KARTĀ"           # Kriyā → Entity (agent)
    - "HAS_KARMA"           # Kriyā → Entity (patient)
    - "USES_KARANA"         # Kriyā → Entity (instrument)
    - "TARGETS_SAMPRADĀNA"  # Kriyā → Entity (recipient)
    - "FROM_APĀDĀNA"        # Kriyā → Entity (source)
    - "LOCATED_IN"          # Kriyā → Entity (spatial location)
    - "OCCURS_AT"           # Kriyā → Entity (temporal location)
    - "IS_SAME_AS"          # Entity → Entity (coreference)
    - "CAUSES"              # Kriyā → Kriyā (causality)
    - "CITED_IN"            # Kriyā → Document (provenance)

# Karaka to Relation Mapping
karaka_relation_mapping:
  KARTA: "HAS_KARTĀ"
  KARMA: "HAS_KARMA"
  KARANA: "USES_KARANA"
  SAMPRADANA: "TARGETS_SAMPRADĀNA"
  APADANA: "FROM_APĀDĀNA"
  ADHIKARANA_SPATIAL: "LOCATED_IN"
  ADHIKARANA_TEMPORAL: "OCCURS_AT"

# GSV-Retry Engine Configuration (Legacy - being phased out)
gsv_retry:
  max_retries: 1  # Reduced: reasoning mode handles verification
  scoring_ensemble_size: 1  # Reduced: single reasoning call is sufficient
  num_candidates: 1  # Reduced: reasoning mode produces better first attempt
  generation_temperature: 0.6  # Per model card for reasoning mode
  scoring_temperature: 0.0
  verification_temperature: 0.6  # Reasoning task

# LLM Call Configuration
llm_call:
  max_tokens: 131072  # Full 128K context window (model supports up to 131,072)
  temperature: 0.6  # Default for reasoning mode
  pad_token_strategy: "use_eos"
  
# Reasoning Mode Configuration
reasoning:
  enabled: true  # Toggle reasoning mode
  mode: "on"  # "on" or "off"
  batch_size: 10  # Process N sentences per LLM call
  use_tool_calling: true  # Use structured tool calling format
  confidence_threshold: 0.85  # Skip verification if confidence > threshold

# Sentence Splitting Pipeline Configuration
sentence_split:
  max_paragraph_tokens: 3500
  chunk_size_tokens: 3000
  overlap_tokens: 200
  max_retries: 2
  llm_max_output_tokens: 4096
  fidelity_threshold: 0.99
  
  # Common abbreviations for sentence boundary detection
  abbreviations:
    - "dr"
    - "mr"
    - "mrs"
    - "ms"
    - "prof"
    - "sr"
    - "jr"
    - "inc"
    - "ltd"
    - "co"
    - "corp"
    - "vs"
    - "etc"

# FAISS Vector Store Configuration
faiss:
  dimension: 2048
  index_type: "IndexFlatL2"
  nearby_k: 5

# Graph Traversal Configuration
graph_traversal:
  max_hops: 3
  default_direction: "both"  # "out", "in", or "both"

# Query Pipeline Configuration
query_pipeline:
  max_reasoning_steps: 10
  citation_limit: 5  # Max citations to display

# Test Queries
test_queries:
  - "Who is Rama?"
  - "What did Rama do?"
  - "Where did the battle take place?"
  - "What caused the war?"

# Logging and Display
display:
  max_text_preview_length: 100
  max_citation_text_length: 80
  show_debug_output: true
  show_verifier_input: true
